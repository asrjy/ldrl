{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrjy/ldrl/blob/main/Chapter%205%20-%20The%20Cross-Entropy%20Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkvdvAMPlxfi"
      },
      "source": [
        "## Cross-Entropy Method\n",
        "\n",
        "Simple method with good convergence. In simple environments that don't require complex, multistep policies, short episodes with frequent rewards, cross entropy works well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEeUTWiNm6y9"
      },
      "source": [
        "### The taxonomy of RL Methods\n",
        "\n",
        "The cross-entropy method falls into the model-free and policy-based category of methods. \n",
        "\n",
        "Many ways to categorize RL methods. But most common are \n",
        "\n",
        "- Model-free or Model-based\n",
        "- Value-based or Policy-based\n",
        "- On-policy or Off-policy\n",
        "\n",
        "Model-free means we don't build a model of the environment or reward. It takes current observatoins, does some computations on them and result is the action it should take. Easier to train as it's hard to build good models of complex environments with rich observations. \n",
        "\n",
        "In Model-based, it tries to predict what the next observation/nextreward will be and based on this prediction it chooses best possible action to take. Often looks more steps into the future. Often seen in deterministic environments, such as board games with strict rules. Only recently, people are combining both of these to get the best of both worlds.\n",
        "\n",
        "Policy-based methods directly approximate the policy of agent ie., what actions agent should carry out at every step. Policy is represented by probability distribution over the available actions. \n",
        "\n",
        "Value-based methods are where the agent calculates the value of every possible action and chooses the action with the best value. Both of these methods are equally popular. \n",
        "\n",
        "Off-policy is the ability of the method to learn on historical data. On-policy requires fresh data to be obtained from the environment. \n",
        "\n",
        "Cross-Entropy is a model-free, policy based and on-policy method of Reinforcement Learning. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZxl8WTqq1_I"
      },
      "source": [
        "### Cross-Entropy method in practice\n",
        "\n",
        "Agent is the most trickiest part of Reinforcement Learning, where it tries to accumulate as much reward as possible. In practice, we replace all of the communication of agent with an ML approach with a non-linear trainable function, which maps the agent's inputs (observations) to some output. \n",
        "\n",
        "In cross-entropy method, a nonlinear function (neural network) produces the policy which tells the agent which action to take for each observation. \n",
        "\n",
        "In practice, the policy is a probability distribution over all actions, which is similar to a classification problem. \n",
        "\n",
        "So, in a sense, the observation passes from the environment to the neural netowrk which gives a probability distribution over actions, performs random sampling using the probability distribution to get an action carried out. This adds randomness to the agent which is a good thing because when the model is initialized, it has random weights. \n",
        "\n",
        "An agent's lifetime is represented using episodes. Each episode is a sequence of observations agent received from the environment, actions it has taken and the rewards for these actions. \n",
        "\n",
        "A discount factor, tells the method how much importance is given to future rewards. Discount factor of 1 means, it's just the sum of all local rewards for every episode. \n",
        "\n",
        "The core of cross-entropy is to throw away bad episodes and train on better ones. The core algorithm is as follows:\n",
        "\n",
        "- Play N number of episodes using our current model and environment. \n",
        "- Calculate total reward for every episode and decide on a threshold. Usually set at 50th or 70th percentile of rewards. \n",
        "- Throw all episodes with rewards under the threshold set. \n",
        "- Train on remaining episodes using observations as input and issued actions as desired output. \n",
        "- Repeat above steps until satisfactory result is obtained. \n",
        "\n",
        "Cross-entropy is quite robust to hyperparameters changing, which makes it an ideal baseline method to try. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KIcj2JbsRyV"
      },
      "source": [
        "### Cross-Entropy on CartPole\n",
        "\n",
        "The NN is a one-hidden-layer NN with ReLU and 128 neurons. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWznUK36vQ6b",
        "outputId": "4ef28d43-0918-4436-c2a5-35674319bcac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "Collecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.4.7-py3-none-win_amd64.whl (22.6 MB)\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "Collecting protobuf>=3.8.0\n",
            "  Downloading protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.22.3-cp39-cp39-win_amd64.whl (14.7 MB)\n",
            "Requirement already satisfied: six in c:\\users\\yash\\.conda\\envs\\ml\\lib\\site-packages (from tensorboardX) (1.16.0)\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py): started\n",
            "  Building wheel for ffmpeg (setup.py): finished with status 'done'\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6084 sha256=769b83f566fcb3568fcbe1f1cd83f5b68d7fe1b114548c15ef3a013a63675bc6\n",
            "  Stored in directory: c:\\users\\yash\\appdata\\local\\pip\\cache\\wheels\\1d\\57\\24\\4eff6a03a9ea0e647568e8a5a0546cdf957e3cf005372c0245\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: protobuf, numpy, tensorboardX, imageio-ffmpeg, ffmpeg\n",
            "Successfully installed ffmpeg-1.4 imageio-ffmpeg-0.4.7 numpy-1.22.3 protobuf-3.20.1 tensorboardX-2.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.16.2 requires pillow>=8.3.2, which is not installed.\n",
            "gym 0.15.4 requires scipy, which is not installed.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX imageio-ffmpeg ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5OSX6p4muTPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import namedtuple\n",
        "import gym\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FvNVN5QvtXdG"
      },
      "outputs": [],
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16 # number of episodes on each iteration\n",
        "PERCENTILE = 70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1oXAmY1ktiT6"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, obs_size, hidden_size, n_actions):\n",
        "    super(Net, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, n_actions)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc5c9fLHulIV"
      },
      "source": [
        "Using a nn.CrossEntropyLoss instead of using Softmax and then calculating Cross Entropy Loss. nn.CrossEntropyLoss requires raw unnormalized data, but we need to apply Softmax on the outputs to get the probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YFcH8WpGuSJj"
      },
      "outputs": [],
      "source": [
        "Episode = namedtuple('Episode', field_names = ['reward', 'steps'])\n",
        "EpisodeStep = namedtuple(\n",
        "    'EpisodeStep', field_names = ['observation', 'action']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRi-LkMcvEda"
      },
      "source": [
        "EpisodeStep will be used to represent on single step agent made in that episode and it stores the output of the step. \n",
        "Episode is a single episode stored as total undiscounted (gamma = 1) and collection of EpisodeSteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dfXy_SvCvjoj"
      },
      "outputs": [],
      "source": [
        "def iterate_batches(env, net, batch_size):\n",
        "  \"\"\"\n",
        "  This takes the gym environment, neural net and the batch size. \n",
        "  The Episode instances are stored in a list. \n",
        "  The reward is tracked. \n",
        "  The environment is reset to obtain the first observation and softmax layer is created to get the prob dists. \n",
        "  \"\"\"\n",
        "  batch = []\n",
        "  episode_reward = 0.0\n",
        "  episode_steps = []\n",
        "  obs = env.reset()\n",
        "  sm = nn.Softmax(dim = 1)\n",
        "  while True:\n",
        "    obs_v = torch.FloatTensor([obs]) # Because all nn.Module instances expect a float tensor. So converting observation to a tensor. \n",
        "    act_probs_v = sm(net(obs_v)) # Getting the probablities using softmax\n",
        "    act_probs = act_probs_v.data.numpy()[0] # Data returned is a tensor. Unpacking it and getting data. \n",
        "    action = np.random.choice(len(act_probs), p = act_probs)\n",
        "    next_obs, reward, is_done, _ = env.step(action)\n",
        "    episode_reward += reward\n",
        "    step = EpisodeStep(observation = obs, action = action)\n",
        "    episode_steps.append(step)\n",
        "    if is_done:\n",
        "      e = Episode(reward = episode_reward, steps = episode_steps)\n",
        "      batch.append(e)\n",
        "      episode_reward = 0.0\n",
        "      episode_steps = []\n",
        "      next_obs = env.reset()\n",
        "      if len(batch) == batch_size:\n",
        "        yield batch\n",
        "        batch = []\n",
        "    obs = next_obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qxTmv9kP5s7v"
      },
      "outputs": [],
      "source": [
        "def filter_batch(batch, percentile):\n",
        "  \"\"\"\n",
        "  This function takes a batch of episodes, uses the percentile value to take the best episodes.\n",
        "  \"\"\"\n",
        "  rewards = list(map(lambda s: s.reward, batch))\n",
        "  reward_bound = np.percentile(rewards, percentile)\n",
        "  reward_mean = float(np.mean(rewards))\n",
        "  train_obs = []\n",
        "  train_act = []\n",
        "  for reward, steps in batch:\n",
        "    if reward < reward_bound:\n",
        "      continue\n",
        "    train_obs.extend(map(lambda step: step.observation, steps))\n",
        "    train_act.extend(map(lambda step: step.action, steps))\n",
        "    train_obs_v = torch.FloatTensor(train_obs)\n",
        "    train_act_v = torch.LongTensor(train_act)\n",
        "    return train_obs_v, train_act_v, reward_bound, reward_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "DfmK-m2h7APt",
        "outputId": "fdae4986-ce76-4bb2-ac85-7f556fe68083"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\ipykernel_6768\\707009024.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
            "  obs_v = torch.FloatTensor([obs]) # Because all nn.Module instances expect a float tensor. So converting observation to a tensor.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 loss = 0.705 reward_mean = 22.8 reward_bound = 24.5\n",
            "1 loss = 0.671 reward_mean = 23.7 reward_bound = 23.5\n",
            "2 loss = 0.657 reward_mean = 39.9 reward_bound = 30.5\n",
            "3 loss = 0.660 reward_mean = 37.9 reward_bound = 34.5\n",
            "4 loss = 0.651 reward_mean = 38.1 reward_bound = 44.0\n",
            "5 loss = 0.625 reward_mean = 32.9 reward_bound = 35.0\n",
            "6 loss = 0.636 reward_mean = 45.9 reward_bound = 64.0\n",
            "7 loss = 0.629 reward_mean = 54.6 reward_bound = 52.0\n",
            "8 loss = 0.670 reward_mean = 48.9 reward_bound = 44.5\n",
            "9 loss = 0.602 reward_mean = 46.7 reward_bound = 53.0\n",
            "10 loss = 0.602 reward_mean = 55.2 reward_bound = 59.0\n",
            "11 loss = 0.618 reward_mean = 65.0 reward_bound = 77.0\n",
            "12 loss = 0.594 reward_mean = 45.3 reward_bound = 46.0\n",
            "13 loss = 0.598 reward_mean = 42.2 reward_bound = 45.0\n",
            "14 loss = 0.585 reward_mean = 50.0 reward_bound = 48.0\n",
            "15 loss = 0.559 reward_mean = 44.9 reward_bound = 50.0\n",
            "16 loss = 0.595 reward_mean = 57.9 reward_bound = 59.0\n",
            "17 loss = 0.589 reward_mean = 70.7 reward_bound = 80.5\n",
            "18 loss = 0.554 reward_mean = 66.3 reward_bound = 80.5\n",
            "19 loss = 0.571 reward_mean = 84.5 reward_bound = 103.5\n",
            "20 loss = 0.569 reward_mean = 84.6 reward_bound = 94.0\n",
            "21 loss = 0.562 reward_mean = 92.5 reward_bound = 99.5\n",
            "22 loss = 0.554 reward_mean = 104.4 reward_bound = 122.5\n",
            "23 loss = 0.564 reward_mean = 99.8 reward_bound = 120.5\n",
            "24 loss = 0.539 reward_mean = 80.2 reward_bound = 87.5\n",
            "25 loss = 0.545 reward_mean = 76.5 reward_bound = 80.0\n",
            "26 loss = 0.543 reward_mean = 69.2 reward_bound = 73.5\n",
            "27 loss = 0.527 reward_mean = 66.8 reward_bound = 81.5\n",
            "28 loss = 0.532 reward_mean = 80.9 reward_bound = 93.0\n",
            "29 loss = 0.516 reward_mean = 75.1 reward_bound = 88.0\n",
            "30 loss = 0.524 reward_mean = 88.1 reward_bound = 108.5\n",
            "31 loss = 0.572 reward_mean = 122.4 reward_bound = 160.0\n",
            "32 loss = 0.527 reward_mean = 160.4 reward_bound = 199.5\n",
            "33 loss = 0.536 reward_mean = 180.9 reward_bound = 200.0\n",
            "34 loss = 0.537 reward_mean = 180.8 reward_bound = 200.0\n",
            "35 loss = 0.507 reward_mean = 168.5 reward_bound = 200.0\n",
            "36 loss = 0.550 reward_mean = 177.8 reward_bound = 200.0\n",
            "37 loss = 0.553 reward_mean = 156.1 reward_bound = 188.5\n",
            "38 loss = 0.512 reward_mean = 135.9 reward_bound = 153.0\n",
            "39 loss = 0.528 reward_mean = 156.5 reward_bound = 200.0\n",
            "40 loss = 0.503 reward_mean = 148.6 reward_bound = 160.0\n",
            "41 loss = 0.507 reward_mean = 171.5 reward_bound = 200.0\n",
            "42 loss = 0.490 reward_mean = 164.7 reward_bound = 200.0\n",
            "43 loss = 0.510 reward_mean = 182.8 reward_bound = 200.0\n",
            "44 loss = 0.505 reward_mean = 176.5 reward_bound = 200.0\n",
            "45 loss = 0.474 reward_mean = 184.3 reward_bound = 200.0\n",
            "46 loss = 0.545 reward_mean = 191.3 reward_bound = 200.0\n",
            "47 loss = 0.484 reward_mean = 192.6 reward_bound = 200.0\n",
            "48 loss = 0.531 reward_mean = 197.8 reward_bound = 200.0\n",
            "49 loss = 0.501 reward_mean = 190.8 reward_bound = 200.0\n",
            "50 loss = 0.517 reward_mean = 198.8 reward_bound = 200.0\n",
            "51 loss = 0.487 reward_mean = 195.3 reward_bound = 200.0\n",
            "52 loss = 0.517 reward_mean = 196.1 reward_bound = 200.0\n",
            "53 loss = 0.442 reward_mean = 199.1 reward_bound = 200.0\n",
            "Solved!\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  env = gym.make(\"CartPole-v0\")\n",
        "  # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
        "  obs_size = env.observation_space.shape[0]\n",
        "  n_actions = env.action_space.n\n",
        "  net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "  objective = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(params = net.parameters(), lr = 0.01)\n",
        "  writer = SummaryWriter(comment = \"-cartpole\")\n",
        "  for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
        "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
        "    optimizer.zero_grad()\n",
        "    action_scores_v = net(obs_v)\n",
        "    loss_v = objective(action_scores_v, acts_v)\n",
        "    loss_v.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"{iter_no} loss = {loss_v.item():.3f} reward_mean = {reward_m:.1f} reward_bound = {reward_b:.1f}\")\n",
        "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "    if reward_m > 199:\n",
        "      print(\"Solved!\")\n",
        "      break\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7JDE_HK9H_3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM5l1eLlMESMBEiv4H3hUwo",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Chapter 5 - The Cross-Entropy Method.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
