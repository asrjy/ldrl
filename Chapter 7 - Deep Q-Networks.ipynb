{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 7 - Deep Q-Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMjLs1399XEHTyjEFPSpnv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrjy/ldrl/blob/main/Chapter%207%20-%20Deep%20Q-Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Networks\n",
        "\n",
        "### Real-life value iteration\n",
        "\n",
        "in both value iteration and it's equivalent q iteration loop over all states and for each state (or state-action pair) calculate the state's value or the q-value. also we assume we know all the states before hand in order to iterate over them. \n",
        "\n",
        "even if we know all states before hand, storing all transitions, their values, the actions and destination states, that's a lot of memory required and iterating over them requires lots of computational power as well. \n",
        "\n",
        "take atari for an example. it has a screen with a resolution of 210x160, each pixel holds one of 128 colors. so every frame of atari has 210x160=33600 pixels, and total number of screens (states) possible = 128 ^ 33600. even with the fastest supercomputer this takes years. and value iteration wants to go over these states just in case. when most of them wont even show up in practical use cases. \n",
        "\n",
        "it also limits us to discrete action spaces. \n"
      ],
      "metadata": {
        "id": "-YACpXKbNDBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tabular Q-Learning\n",
        "\n",
        "intuition behind this is that we dont really need all of the states in the environment. we just need the ones obtained by interacting with the environment. if a state space is not shown to us by the environment, we don't really care about it's value. \n",
        "\n",
        "this modification to the value iteration method is called Q-learning. it works in the following way:\n",
        "- start with empty table. mapping states with values of actions.\n",
        "- obtain the current state (s), action performed (a), reward obtained for the action (r) and the new state (s') by interacting with the environment. the way we pick the action is not confined to any method, in this step. \n",
        "- update Q(s, a) value using the bellman approximation\n",
        "\n",
        "    ![bellman](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_06_001.png)\n",
        "\n",
        "repeat the above two steps until a threshold is reached where there is not much update in the bellman update, or we could stop after a number and run a test episode to get the reward. \n",
        "\n",
        "in the above algorithm, we update the q value by taking samples from the environment and assign new values and take samples from the environment again. this is a bad idea and could lead to unstable training. so we implement a learning rate based approach by changing the update equation to \n",
        "![tabular q learning rate](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_06_003.png)\n",
        "\n",
        "this allows values of q to converge smoothly even if environment is noisy. \n",
        "\n",
        "### Tabular-Q Learning on FrozenLake\n"
      ],
      "metadata": {
        "id": "lt72KDwmbY1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2r4muyCvcZ2",
        "outputId": "af27939c-07e0-407f-9c8a-0e639cd4cee3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter"
      ],
      "metadata": {
        "id": "5LBk6M0zvgzI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.2 # Learning Rate\n",
        "TEST_EPISODES = 20"
      ],
      "metadata": {
        "id": "mMKn9iNsvk_m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self.env = gym.make(ENV_NAME)\n",
        "    self.state = self.env.reset()\n",
        "    self.values = collections.defaultdict(float)\n",
        "  \n",
        "  def sample_env(self):\n",
        "    \"\"\"\n",
        "    Performs a random action on the environment and returns old state (s), action taken (a), reward(r)\n",
        "    and new state (s')\n",
        "    \"\"\"\n",
        "    action = self.env.action_space.sample()\n",
        "    old_state = self.state\n",
        "    new_state, reward, is_done, _ = self.env.step(action)\n",
        "    self.state = self.env.reset() if is_done else new_state\n",
        "    return old_state, action, reward, new_state \n",
        "\n",
        "  def best_value_and_action(self, state):\n",
        "    \"\"\"\n",
        "    This method takes the state of the environment and picks the best action to take in this state\n",
        "    by choosing the action with the largest value. If there is no value for the state action pair in \n",
        "    the value table, it's value is taken as 0. This situation arises twice:\n",
        "      - in the first test episode \n",
        "      - in the method that performs value update to get value of the next state\n",
        "    \"\"\"\n",
        "    best_value, best_action = None, None\n",
        "    for action in range(self.env.action_space.n):\n",
        "      action_value = self.values[(state, action)]\n",
        "      if best_value is None or best_value < action_value:\n",
        "        best_value = action_value\n",
        "        best_action = action \n",
        "    return best_value, best_action\n",
        "  \n",
        "  def value_update(self, s, a, r, next_s):\n",
        "    \"\"\"\n",
        "    Performing bellman approximation from our state s, action a, reward r, next state next_state\n",
        "    \"\"\"\n",
        "    best_v, _ = self.best_value_and_action(next_s)\n",
        "    new_v = r + GAMMA * best_v\n",
        "    old_v = self.values[(s, a)]\n",
        "    self.values[(s, a)] = old_v * (1 - ALPHA) + new_v * ALPHA\n",
        "  \n",
        "  def play_episode(self, env):\n",
        "    \"\"\"\n",
        "    Plays one full episode using the provided test environment. action is taken based on the current value\n",
        "    of q table. this method is used to evaluate the current policy and check progress of learning. \n",
        "    this method does not alter the value table. only uses it to find the best action made. \n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "      _, action = self.best_value_and_action(state)\n",
        "      new_state, reward, is_done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      if is_done:\n",
        "        break\n",
        "      state = new_state\n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "RnkH9c8DvtQ_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  test_env = gym.make(ENV_NAME)\n",
        "  agent = Agent()\n",
        "  writer = SummaryWriter(comment = '-q-learning')\n",
        "  iter_no = 0\n",
        "  best_reward = 0.0\n",
        "  while True:\n",
        "    iter_no += 1\n",
        "    s, a, r, next_s = agent.sample_env()\n",
        "    agent.value_update(s, a, r, next_s)\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "      reward += agent.play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "      print(f\"Best reward updated: {best_reward:.3f} -> {reward:.3f}\")\n",
        "      best_reward = reward \n",
        "    if reward > 0.80:\n",
        "      print(f\"Solved in {iter_no} iterations!\")\n",
        "      break\n",
        "  writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_CsTtkXtq6v",
        "outputId": "dac57d60-79ce-46c1-baa3-bff242225906"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated: 0.000 -> 0.050\n",
            "Best reward updated: 0.050 -> 0.100\n",
            "Best reward updated: 0.100 -> 0.150\n",
            "Best reward updated: 0.150 -> 0.200\n",
            "Best reward updated: 0.200 -> 0.250\n",
            "Best reward updated: 0.250 -> 0.350\n",
            "Best reward updated: 0.350 -> 0.400\n",
            "Best reward updated: 0.400 -> 0.550\n",
            "Best reward updated: 0.550 -> 0.800\n",
            "Best reward updated: 0.800 -> 0.900\n",
            "Solved in 6142 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Q-Learning\n",
        "\n",
        "above approach solves the issue of going through all  states in state space by only going through observed states. but it could still be an issue when thenumber of observable states is very large. in some environments, the observable states could be infinite (continuous states)\n",
        "\n",
        "we could build a neural net that minimizes loss as a function of Q-function as follows:\n",
        "- initialize Q(s, a) with some initial approximation\n",
        "- interact with environment and obtain the tuple (s, a, r, s')\n",
        "- calculate loss L = (Q(s, a) - r)^2 if episode has ended, or ![loss when episode has not ended](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_06_007.png) if episode has not ended. \n",
        "- minimize this loss using SGD and update Q(s, a)\n",
        "- repeat from step 2 till convergence. \n",
        "\n",
        "#### Interaction with the Environment\n",
        "\n",
        "issue with the above approach is that it needs to interact with the environment for the model to be trained. this works fine with small environments like frozen lake where we can take random actions and survive but for complex environments like pong, this is not possible. as an alternative, we could use the q-function approximation as a source of behaviour. \n",
        "\n",
        "but if our q function approximation is not good/perfect, then the agent will be stuck with bad actions and in some states it wont even behave differently. \n",
        "\n",
        "this is the explore-exploit dilemma faced in reinforcement learning. on one hand the agent needs to explore the environment to get a complete picture of the transition table and on the other hand we shouldnt waste time by randomly trying actions we have already tried and know the outcomes of. \n",
        "\n",
        "a method that performs such a mix of extreme behaviours is known as epsilon-greedy method. we start off with setting epsilon value to 1, which means 100% random actions and slowly reduce it's value to 0.05 which means 5% random actions. there are other solutions apart from epsilon greedy and this problem is one of the fundamental questions in RL problems. \n",
        "\n",
        "\n",
        "#### SGD Optimization\n",
        "one of the fundamental requirements of SGD is that the training data be independent and identically distributed (iid data). but the data provided in our current situation does not satisfy both conditions \n",
        "- the traning data is not independent. even if we accumulate large number of samples, they will all be very close to each other as they belong to the same episode. \n",
        "- the training data is not similar to the data provided by the optimal policy we want to learn because the data will be the result of same other policy that's not optimal (could be random or epsilon-greedy). we dont want to learn a random policy, we want to learn the policy that gives the most reward. \n",
        "\n",
        "to deal with this, we use replay buffer where new training data from our latest experience and add it at the end of training data, removing the same amount of oldest data. \n",
        "\n",
        "#### Correlation between stpes\n",
        "another issue with the lack of iid data is that, we are updating the value of Q(s, a) using Q(s', a'). they only have one step between them making them very similar. it's hard for the NNs to distinguish between them and when we try to alter the neural net's parameters to make Q(s, a) closer to the desired ones, it could indirectly alter the value produced by Q(s', a') or any other states nearby. this makes our training very unstable like chasing our own tail. \n",
        "\n",
        "a workaround for this could be, we keep a copy of our network and use it for the Q(s', a') value in the bellman equation. this network is synchronized with our main network only periodically. this is called target network. \n",
        "\n",
        "#### The Markov Property\n",
        "a fundamental assumption made in our rl approach is that each observations from the environment are independent with each other. this is not usually true. for example, a pong screenshot is not interpretable without preceeding few screenshots. these sort of problems fall into the area of partially observable MDPs (POMDPs). another example of POMDPs is a card game where you don't see your opponent's cards, because the cards you have and cards on table could correspond to different cards in your opponent's hands (because you might get his cards, or he might get cards that were on table etc.,)\n",
        "\n",
        "one workaround to this is using a set of observations as one observation. for example, in case of pong, we stack k subsequent frames and use them as observation at every state. the classic number of observations stack in atari is 4. \n",
        "\n",
        "#### Final form of DQN Training\n",
        "there are many tricks and hacks to overcome problems faced in dqn models, but just epsilon-greedy, replay buffer and target network allowed DeepMind to succesfully train DQNs on 49 Atari games. \n",
        "\n",
        "the dqn algorithm for the above is as follows:\n",
        "- initialize parameters for Q(s, a) and Q'(s, a) with random weights, epsilon = 1 and empty replay buffer\n",
        "- with probability epsilon, select a random action a, otherwise a = argmax a [Q(s, a)]\n",
        "- execute action a in emulator and observe reward r, and next state s'. \n",
        "- store transition (s, a, r, s') in replay buffer\n",
        "- sample a random mini-batch of transitions from the replay buffer\n",
        "- for every transtion from the mini batch, calculate target value y = r (reward) if the episode has ended or \n",
        "![y if episode has not ended](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_06_016.png)\n",
        "- calculate loss as L = (Q(s, a) - y)^2 \n",
        "- update Q(s, a) using SGD by minimizing loss \n",
        "- every N steps, copy weights from Q to Q'\n",
        "- repeat till convergence"
      ],
      "metadata": {
        "id": "TRk7fk6EvOwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN on Pong\n",
        "\n",
        "even though rl models are not as compute hungry as state of the art image net models, dqn models from 2015 have 1.5m models. so care should be taken not to copy weights frequentyl in target network etc., a naive version of dqn that iterates over each sample of mini batch is twice as slow as the parallel version. a single extra copy of data batch could make it 13 times slower. \n",
        "\n",
        "#### Wrappers\n"
      ],
      "metadata": {
        "id": "5YJSUGPzvqVT"
      }
    }
  ]
}