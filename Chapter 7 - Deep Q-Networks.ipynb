{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 7 - Deep Q-Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlYv9fXglS/HiJPe3ZnVUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrjy/ldrl/blob/main/Chapter%207%20-%20Deep%20Q-Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Networks\n",
        "\n",
        "### Real-life value iteration\n",
        "\n",
        "in both value iteration and it's equivalent q iteration loop over all states and for each state (or state-action pair) calculate the state's value or the q-value. also we assume we know all the states before hand in order to iterate over them. \n",
        "\n",
        "even if we know all states before hand, storing all transitions, their values, the actions and destination states, that's a lot of memory required and iterating over them requires lots of computational power as well. \n",
        "\n",
        "take atari for an example. it has a screen with a resolution of 210x160, each pixel holds one of 128 colors. so every frame of atari has 210x160=33600 pixels, and total number of screens (states) possible = 128 ^ 33600. even with the fastest supercomputer this takes years. and value iteration wants to go over these states just in case. when most of them wont even show up in practical use cases. \n",
        "\n",
        "it also limits us to discrete action spaces. \n"
      ],
      "metadata": {
        "id": "-YACpXKbNDBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tabular Q-Learning\n",
        "\n",
        "intuition behind this is that we dont really need all of the states in the environment. we just need the ones obtained by interacting with the environment. if a state space is not shown to us by the environment, we don't really care about it's value. \n",
        "\n",
        "this modification to the value iteration method is called Q-learning. it works in the following way:\n",
        "- start with empty table. mapping states with values of actions.\n",
        "- obtain the current state (s), action performed (a), reward obtained for the action (r) and the new state (s') by interacting with the environment. the way we pick the action is not confined to any method, in this step. \n",
        "- update Q(s, a) value using the bellman approximation\n",
        "\n",
        "    ![bellman](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_06_001.png)\n",
        "\n",
        "repeat the above two steps until a threshold is reached where there is not much update in the bellman update, or we could stop after a number and run a test episode to get the reward. \n",
        "\n",
        "in the above algorithm, we update the q value by taking samples from the environment and assign new values and take samples from the environment again. this is a bad idea and could lead to unstable training. so we implement a learning rate based approach by changing the update equation to \n",
        "![tabular q learning rate](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_06_003.png)\n",
        "\n",
        "this allows values of q to converge smoothly even if environment is noisy. \n",
        "\n",
        "### Tabular-Q Learning on FrozenLake\n"
      ],
      "metadata": {
        "id": "lt72KDwmbY1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2r4muyCvcZ2",
        "outputId": "af27939c-07e0-407f-9c8a-0e639cd4cee3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter"
      ],
      "metadata": {
        "id": "5LBk6M0zvgzI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "ALPHA = 0.2 # Learning Rate\n",
        "TEST_EPISODES = 20"
      ],
      "metadata": {
        "id": "mMKn9iNsvk_m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self.env = gym.make(ENV_NAME)\n",
        "    self.state = self.env.reset()\n",
        "    self.values = collections.defaultdict(float)\n",
        "  \n",
        "  def sample_env(self):\n",
        "    \"\"\"\n",
        "    Performs a random action on the environment and returns old state (s), action taken (a), reward(r)\n",
        "    and new state (s')\n",
        "    \"\"\"\n",
        "    action = self.env.action_space.sample()\n",
        "    old_state = self.state\n",
        "    new_state, reward, is_done, _ = self.env.step(action)\n",
        "    self.state = self.env.reset() if is_done else new_state\n",
        "    return old_state, action, reward, new_state \n",
        "\n",
        "  def best_value_and_action(self, state):\n",
        "    \"\"\"\n",
        "    This method takes the state of the environment and picks the best action to take in this state\n",
        "    by choosing the action with the largest value. If there is no value for the state action pair in \n",
        "    the value table, it's value is taken as 0. This situation arises twice:\n",
        "      - in the first test episode \n",
        "      - in the method that performs value update to get value of the next state\n",
        "    \"\"\"\n",
        "    best_value, best_action = None, None\n",
        "    for action in range(self.env.action_space.n):\n",
        "      action_value = self.values[(state, action)]\n",
        "      if best_value is None or best_value < action_value:\n",
        "        best_value = action_value\n",
        "        best_action = action \n",
        "    return best_value, best_action\n",
        "  \n",
        "  def value_update(self, s, a, r, next_s):\n",
        "    \"\"\"\n",
        "    Performing bellman approximation from our state s, action a, reward r, next state next_state\n",
        "    \"\"\"\n",
        "    best_v, _ = self.best_value_and_action(next_s)\n",
        "    new_v = r + GAMMA * best_v\n",
        "    old_v = self.values[(s, a)]\n",
        "    self.values[(s, a)] = old_v * (1 - ALPHA) + new_v * ALPHA\n",
        "  \n",
        "  def play_episode(self, env):\n",
        "    \"\"\"\n",
        "    Plays one full episode using the provided test environment. action is taken based on the current value\n",
        "    of q table. this method is used to evaluate the current policy and check progress of learning. \n",
        "    this method does not alter the value table. only uses it to find the best action made. \n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "      _, action = self.best_value_and_action(state)\n",
        "      new_state, reward, is_done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      if is_done:\n",
        "        break\n",
        "      state = new_state\n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "RnkH9c8DvtQ_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  test_env = gym.make(ENV_NAME)\n",
        "  agent = Agent()\n",
        "  writer = SummaryWriter(comment = '-q-learning')\n",
        "  iter_no = 0\n",
        "  best_reward = 0.0\n",
        "  while True:\n",
        "    iter_no += 1\n",
        "    s, a, r, next_s = agent.sample_env()\n",
        "    agent.value_update(s, a, r, next_s)\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "      reward += agent.play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "      print(f\"Best reward updated: {best_reward:.3f} -> {reward:.3f}\")\n",
        "      best_reward = reward \n",
        "    if reward > 0.80:\n",
        "      print(f\"Solved in {iter_no} iterations!\")\n",
        "      break\n",
        "  writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_CsTtkXtq6v",
        "outputId": "dac57d60-79ce-46c1-baa3-bff242225906"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated: 0.000 -> 0.050\n",
            "Best reward updated: 0.050 -> 0.100\n",
            "Best reward updated: 0.100 -> 0.150\n",
            "Best reward updated: 0.150 -> 0.200\n",
            "Best reward updated: 0.200 -> 0.250\n",
            "Best reward updated: 0.250 -> 0.350\n",
            "Best reward updated: 0.350 -> 0.400\n",
            "Best reward updated: 0.400 -> 0.550\n",
            "Best reward updated: 0.550 -> 0.800\n",
            "Best reward updated: 0.800 -> 0.900\n",
            "Solved in 6142 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Q-Learning\n",
        "\n",
        "above approach solves the issue of going through all  states in state space by only going through observed states. but it could still be an issue when thenumber of observable states is very large. in some environments, the observable states could be infinite (continuous states)\n",
        "\n",
        "we could build a neural net that minimizes loss as a function of Q-function as follows:\n",
        "- initialize Q(s, a) with some initial approximation\n",
        "- interact with environment and obtain the tuple (s, a, r, s')\n",
        "- calculate loss L = (Q(s, a) - r)^2 if episode has ended, or ![loss when episode has not ended](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_06_007.png) if episode has not ended. \n",
        "- minimize this loss using SGD and update Q(s, a)\n",
        "- repeat from step 2 till convergence. \n",
        "\n",
        "#### Interaction with the Environment\n",
        "\n",
        "issue with the above approach is that it needs to interact with the environment for the model to be trained. this works fine with small environments like frozen lake where we can take random actions and survive but for complex environments like pong, this is not possible. as an alternative, we could use the q-function approximation as a source of behaviour. \n",
        "\n",
        "but if our q function approximation is not good/perfect, then the agent will be stuck with bad actions and in some states it wont even behave differently. \n",
        "\n",
        "this is the explore-exploit dilemma faced in reinforcement learning. on one hand the agent needs to explore the environment to get a complete picture of the transition table and on the other hand we shouldnt waste time by randomly trying actions we have already tried and know the outcomes of. \n",
        "\n",
        "a method that performs such a mix of extreme behaviours is known as epsilon-greedy method. we start off with setting epsilon value to 1, which means 100% random actions and slowly reduce it's value to 0.05 which means 5% random actions. there are other solutions apart from epsilon greedy and this problem is one of the fundamental questions in RL problems. \n",
        "\n",
        "\n",
        "#### SGD Optimization\n",
        "one of the fundamental requirements of SGD is that the training data be independent and identically distributed (iid data). but the data provided in our current situation does not satisfy both conditions \n",
        "- the traning data is not independent. even if we accumulate large number of samples, they will all be very close to each other as they belong to the same episode. \n",
        "- the training data is not similar to the data provided by the optimal policy we want to learn because the data will be the result of same other policy that's not optimal (could be random or epsilon-greedy). we dont want to learn a random policy, we want to learn the policy that gives the most reward. \n",
        "\n",
        "to deal with this, we use replay buffer where new training data from our latest experience and add it at the end of training data, removing the same amount of oldest data. \n",
        "\n",
        "#### Correlation between stpes\n",
        "another issue with the lack of iid data is that, we are updating the value of Q(s, a) using Q(s', a'). they only have one step between them making them very similar. it's hard for the NNs to distinguish between them and when we try to alter the neural net's parameters to make Q(s, a) closer to the desired ones, it could indirectly alter the value produced by Q(s', a') or any other states nearby. this makes our training very unstable like chasing our own tail. \n",
        "\n",
        "a workaround for this could be, we keep a copy of our network and use it for the Q(s', a') value in the bellman equation. this network is synchronized with our main network only periodically. this is called target network. \n",
        "\n",
        "#### The Markov Property\n",
        "a fundamental assumption made in our rl approach is that each observations from the environment are independent with each other. this is not usually true. for example, a pong screenshot is not interpretable without preceeding few screenshots. these sort of problems fall into the area of partially observable MDPs (POMDPs). another example of POMDPs is a card game where you don't see your opponent's cards, because the cards you have and cards on table could correspond to different cards in your opponent's hands (because you might get his cards, or he might get cards that were on table etc.,)\n",
        "\n",
        "one workaround to this is using a set of observations as one observation. for example, in case of pong, we stack k subsequent frames and use them as observation at every state. the classic number of observations stack in atari is 4. \n",
        "\n",
        "#### Final form of DQN Training\n",
        "there are many tricks and hacks to overcome problems faced in dqn models, but just epsilon-greedy, replay buffer and target network allowed DeepMind to succesfully train DQNs on 49 Atari games. \n",
        "\n",
        "the dqn algorithm for the above is as follows:\n",
        "- initialize parameters for Q(s, a) and Q'(s, a) with random weights, epsilon = 1 and empty replay buffer\n",
        "- with probability epsilon, select a random action a, otherwise a = argmax a [Q(s, a)]\n",
        "- execute action a in emulator and observe reward r, and next state s'. \n",
        "- store transition (s, a, r, s') in replay buffer\n",
        "- sample a random mini-batch of transitions from the replay buffer\n",
        "- for every transtion from the mini batch, calculate target value y = r (reward) if the episode has ended or \n",
        "![y if episode has not ended](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_06_016.png)\n",
        "- calculate loss as L = (Q(s, a) - y)^2 \n",
        "- update Q(s, a) using SGD by minimizing loss \n",
        "- every N steps, copy weights from Q to Q'\n",
        "- repeat till convergence"
      ],
      "metadata": {
        "id": "TRk7fk6EvOwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN on Pong\n",
        "\n",
        "even though rl models are not as compute hungry as state of the art image net models, dqn models from 2015 have 1.5m models. so care should be taken not to copy weights frequentyl in target network etc., a naive version of dqn that iterates over each sample of mini batch is twice as slow as the parallel version. a single extra copy of data batch could make it 13 times slower. \n",
        "\n",
        "#### Wrappers\n",
        "tracking atari games in rl is quite demanding. so openai includes gym wrappers that are transformations that influence only performance and address atari platform features that making learning long and unstable. \n",
        "\n",
        "some helpful wrappers used are:\n",
        "- converting individual lives in games to seperate episodes. only useful on some environments. \n",
        "- performing a random amount of no-op actions at the beginning of each episode to skip intros etc.,\n",
        "- making an action decision every k steps instead of every single step. useful to reduce computational power required and helps in games that require past few frames to make a good decision.\n",
        "- taking maximum of two pixels in the last two frames as some atari games have a flicker effect. these are not visible to human eye but could confuse the neural nets. \n",
        "- pressing FIRE at the beginning of game. some games require us to press FIRE to begin. \n",
        "- scaling every frame down from 210x160 three colors to single colored 84x84. some researchers do grayscale, some change y-color channel to ycbcr. \n",
        "- clipping reward to -1, 0, 1values. different games can have varying score scales. \n",
        "- converting observations from unsigned bytes to float32. \n",
        "\n",
        "we don't really need all of these wrappers for simple pong. sometimes when dqn is not converging, problem could be in wrongly wrapped environment. "
      ],
      "metadata": {
        "id": "5YJSUGPzvqVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import gym\n",
        "import gym.spaces\n",
        "import numpy as np\n",
        "import collections\n"
      ],
      "metadata": {
        "id": "122Mz_SGFEkf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  This wrapper presses FIRE button in environments that require game to start. In addition to pressing FIRE,\n",
        "  this wrapper checks for some corner cases present in some games. \n",
        "  \"\"\"\n",
        "  def __init__(self, env = None):\n",
        "    super(FireResetEnv, self).__init__(env)\n",
        "    assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
        "    assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "  def step(self, action):\n",
        "    return self.env.step(action)\n",
        "  \n",
        "  def reset(self):\n",
        "    self.env.reset()\n",
        "    obs, _, done, _ = self.env.step(1)\n",
        "    if done:\n",
        "      self.env.reset()\n",
        "    obs, _, done, _ = self.env.step(2)\n",
        "    if done:\n",
        "      self.env.reset()\n",
        "    return obs"
      ],
      "metadata": {
        "id": "4K1cc3hUFIl4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  Combines the repitition of actions during k frames and pixels from two consecutive frames\n",
        "  \"\"\"\n",
        "  def __init__(self, env=None, skip = 4):\n",
        "    super(MaxAndSkipEnv, self).__init__(env)\n",
        "    self._obs_buffer = collections.deque(maxlen = 2)\n",
        "    self._skip = skip\n",
        "  def step(self, action):\n",
        "    total_reward = 0.0\n",
        "    done = None \n",
        "    for _ in range(self._skip):\n",
        "      obs, reward, done, info = self.env.step(action)\n",
        "      self._obs_buffer.append(obs)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "    return max_frame, total_reward, done, info \n",
        "  def reset(self):\n",
        "    self._obs_buffer.clear()\n",
        "    obs = self.env.reset()\n",
        "    self._obs_buffer.append(obs)\n",
        "    return obs"
      ],
      "metadata": {
        "id": "clabmkEEGAso"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "  \"\"\"\n",
        "  converts color 210x160 image to grayscale 84x84. conversion to grayscale is done using colorimetric grayscale\n",
        "  conversion which is closer to human color perception than simple averaging of color channels. converts to\n",
        "  grayscale, resizes image to 84x84, crops top and bottom parts as a result.\n",
        "  \"\"\"\n",
        "  def __init__(self, env = None):\n",
        "    super(ProcessFrame84, self).__init__(env)\n",
        "    self.observation_space = gym.spaces.Box(low = 0, high = 255, shape = (84, 84, 1), dtype = np.uint8)\n",
        "\n",
        "  def observation(self, obs):\n",
        "    return ProcessFrame84.process(obs)\n",
        "\n",
        "  @staticmethod\n",
        "\n",
        "  def process(frame):\n",
        "    if frame.size == 210 * 160 * 3:\n",
        "      img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "    elif frame.size == 250 * 160 * 3:\n",
        "      img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "    else:\n",
        "      assert False, \"Unknown resolution.\"\n",
        "    img = img[:, :, 0] * 0.229 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "    resized_screen = cv2.resize(img, (84, 110), interpolation = cv2.INTER_AREA)\n",
        "    x_t = resized_screen[18:102, :]\n",
        "    x_t = np.reshape(x_t, [94, 84, 1])\n",
        "    return x_t.astype(np.unit8)"
      ],
      "metadata": {
        "id": "V59Egym3JteO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "  \"\"\"\n",
        "  creats a stack of subsequent frames along the first dimension and returns them as an observation. \n",
        "  \"\"\"\n",
        "  def __init__(self, env, n_steps, dtype = np.float32):\n",
        "    super(BufferWrapper, self).__init__(env)\n",
        "    self.dtype = dtype\n",
        "    old_space = env.observation_space\n",
        "    self.observation_space = gym.spaces.Box(\n",
        "        old_space.low.repeat(n_steps, axis = 0)\n",
        "        old_space.high.repeat(n_steps, axis = 0), dtype = dtype)\n",
        "  \n",
        "  def reset(self):\n",
        "    self.buffer = np.zeros_like(\n",
        "        self.observation_space.low, dtype = self.dtype\n",
        "    )\n",
        "    return self.observation(self.env.reset())\n",
        "  \n",
        "  def observation(self, observation):\n",
        "    self.buffer[:-1] = self.buffer[1:]\n",
        "    self.buffer[-1] = observation\n",
        "    return self.buffer"
      ],
      "metadata": {
        "id": "MJ2VkfWJLKz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "  \"\"\"\n",
        "  this wrapper changes the observation from HWC(height, width, channel) to CHW(channel, height, width) \n",
        "  required by pytorch. \n",
        "  \"\"\"\n",
        "  def __init__(self, env):\n",
        "    super(ImageToPyTorch, self).__init__(env)\n",
        "    old_shape = self.observation_space.shape\n",
        "    new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n",
        "    self.observation_space = gym.spaces.Box(\n",
        "        low = 0.0, high = 1.0, shape = new_shape, dtype = np.float32\n",
        "    )\n",
        "  \n",
        "  def observation(self, observation):\n",
        "    return np.moveaxis(observation, 2, 0)"
      ],
      "metadata": {
        "id": "5H9sKqCSoOWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "  \"\"\"\n",
        "  this wrapper converts observation data from byte to float and sclaes every pixel value between 0.0 and 1.0\n",
        "  \"\"\"\n",
        "  def observation(self, obs):\n",
        "    return np.array(obs).astype(np.float32)/255.0"
      ],
      "metadata": {
        "id": "gXygq8Ylo8px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(env_name):\n",
        "  \"\"\"\n",
        "  This function creates the environment and applies all wrappers on it\n",
        "  \"\"\"\n",
        "  env = gym.make(env_name)\n",
        "  env = MaxAndSkipEnv(env)\n",
        "  env = FireResetEnv(env)\n",
        "  env = ProcessFrame84(env)\n",
        "  env = ImageToPyTorch(env)\n",
        "  env = BufferWrapper(env, 4)\n",
        "  return ScaledFloatFrame(env)"
      ],
      "metadata": {
        "id": "bExbj4vUpURI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The DQN Model"
      ],
      "metadata": {
        "id": "KWeZErv-ppvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "m2ASvELcp3_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, input_shape, n_actions):\n",
        "    super(DQN, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "      nn.Conv2d(input_shape[0], 32, kernel_size = 8, stride = 4),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 64, kernel_size = 4, stride = 2),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size = 3, stride = 1),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "    conv_out_size = self._get_conv_out(input_shape)\n",
        "    self.fc = nn.Sequential(\n",
        "      nn.Linear(conv_out_size, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, n_actions)\n",
        "    )\n",
        "  \n",
        "  def _get_conv_out(self, shape):\n",
        "    \"\"\"\n",
        "    we dont know the exact number of values in the output from convolution layer produced from the input shape.\n",
        "    since we need to pass this number to the fully connected layer, this function creates a fake tensor of such\n",
        "    shape and returns the number of parameters. \n",
        "    \"\"\"\n",
        "    o = self.conv(torch.zeros(1, *shape))\n",
        "    return int(np.prod(o.size))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    pytorch does not have a flatten layer to flatten the 3d convoluted output to the fully connected layer.\n",
        "    so this function does a forward pass using both conv net and fully connected network.\n",
        "    the conv output is a 4d tensor. the first dimension is the batch size, second is the color channel which\n",
        "    is our stack of subsequent frames, third and fourth are image dimensions,\n",
        "    .view() allows us to reshape without creating a new memory object. \n",
        "    \"\"\"\n",
        "    conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "    return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "JIxmiXfkp60K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training\n"
      ],
      "metadata": {
        "id": "md1fmaLguyA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "Ix0xOz8Xxa_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lib import wrappers\n",
        "from lib import dqn_model\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tensorboardX import SummaryWriter"
      ],
      "metadata": {
        "id": "R0WGDvz3xRyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19.0"
      ],
      "metadata": {
        "id": "rzDKmCg9xfIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32 # Batch Size sampled from replay buffer\n",
        "REPLAY_SIZE = 10000 # Maximum Capacity of the buffer\n",
        "REPLAY_START_SIZE = 10000 # Count of frames we wait before starting training to populate buffer\n",
        "LEARNING_RATE = 1e-4 # Learning Rate for Adam Optimizer\n",
        "SYNC_TARGET_FRAMES = 1000 # Frequecy of model weight sync from training model to target model"
      ],
      "metadata": {
        "id": "xZ4GZETdxlSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPSILON_DECAY_LAST_FRAME = 150000 # During first 150000 frames, epsilon is linearly reduced to 0.01.\n",
        "# In the original paper this value was 1000000 (1 Million)\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01"
      ],
      "metadata": {
        "id": "CwlUmSVRxuCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Experience = collections.namedtuple('Experience', field_names = ['state', 'action', 'reward', 'done', 'new_state'])"
      ],
      "metadata": {
        "id": "I7f6NIwZyOBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = collections.dequeu(maxlen=capacity)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  def sample(self, batch_size):\n",
        "    indices = np.random.choice(len(self.buffer), batch_size, replace = False)\n",
        "    states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "    return np.array(states), np.array(actions), np.array(rewards, dtype = np.float32), np.array(dones, dtype = np.uint8), np.array(next_states)    "
      ],
      "metadata": {
        "id": "oP2QD6lvymWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, env, exp_buffer):\n",
        "    self.env = env\n",
        "    self.exp_buffer = exp_buffer\n",
        "    self._reset()\n",
        "  \n",
        "  def _reset(self):\n",
        "    self.state = env.reset()\n",
        "    self.total_reward = 0.0\n",
        "  \n",
        "  @torch.no_grad()\n",
        "\n",
        "  def play_step(self, net, epsilon = 0.0, device = 'cpu'):\n",
        "    \"\"\"\n",
        "    Plays a step in the environment and stores its result in the buffer. Using epsilon, we eigher take a random\n",
        "    step or use past model to obtain q values of all possible actions and choose the best\n",
        "    \"\"\"\n",
        "    done_reward = None\n",
        "    if np.random.random() < epsilon:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      state_a = np.array([self.state], copy = False)\n",
        "      state_v = torch.tensor(state_a).to(device)\n",
        "      q_vals_v = net(state_v)\n",
        "      _, act_v = torch.max(q_vals, dim = 1)\n",
        "      action = int(act_v.item())\n",
        "    new_state, reward, is_done, _ = self.env.step(action)\n",
        "    self.total_reward += reward\n",
        "    exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "    self.exp_buffer.append(exp)\n",
        "    self.state = new_state\n",
        "    if is_done:\n",
        "      done_reward = self.total_reward\n",
        "      self._reset()\n",
        "    return done_reward\n",
        "  \n",
        "  def calc_loss(batch, net, tgt_net, device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Calculates the loss for the sampled batch. \n",
        "    batch: typle of arrays repacked by sample() in the experience buffer\n",
        "    net: used to calculate gradients\n",
        "    tgt_net: used to calculate values for the next states and this won't affect gradients. We use detach()\n",
        "    to prevent gradients from flowing into the target network's graph. \n",
        "    \"\"\"\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    states_v = torch.tensor(np.array(states, copy = false)).to(device)\n",
        "    next_states_v = torch.tensor(np.array(next_states, copy = False)).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(dones).to(device)\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    \n"
      ],
      "metadata": {
        "id": "CeFNFsmNzNjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}