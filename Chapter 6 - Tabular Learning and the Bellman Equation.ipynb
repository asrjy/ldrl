{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrjy/ldrl/blob/main/Chapter%204%20-%20Tabular%20Learning%20and%20the%20Bellman%20Equation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI67L1UGs0FD"
      },
      "source": [
        "## Tabular Learning and the Bellman Equation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxNThE-k8VJN"
      },
      "source": [
        "The value of a state has been defined as, the expected total reward (discounted or not) from the state. \n",
        "ie., \n",
        "\n",
        "value = (sum of all rewards * discount factor ^ t) for all t\n",
        "\n",
        "the optimal policy is the one which gives the most reward as possible, by extension the best value of each state. \n",
        "\n",
        "such simple environments with an obvious policy are not interesting and not generally seen in real life. for the interesting environments, optimal policies are hard to devise and even harder to prove their optimality. \n",
        "\n",
        "its not always good to be \"greedy\" ie., taking the action with the highest reward. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq_6jwOt9Kgs"
      },
      "source": [
        "### The Bellman equation of optimality\n",
        "\n",
        "the approach may look similar to the greedy approach but the key difference is that we not only look at the immediate reward, but also the value of the state that lets us know about the future rewards. \n",
        "\n",
        "so to get the expected value, \n",
        "\n",
        "E = for all states(probability of each action possible at that state * (reward for that action + (gamma * value of the state we end up in)))\n",
        "\n",
        "using this, the Bellman Optimality Equation can be defined for a general case (both stochastic and deterministic) as\n",
        "\n",
        "![image.png](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_05_010.png)\n",
        "\n",
        "P (a, i -> j) means probabilty of action a, issued in state i, ending up in state j.\n",
        "\n",
        "the equation is recursive in nature (the value of the state is defined using the value of target states), we pretend we know the values by randomly initializing them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTpoXssOPg_q"
      },
      "source": [
        "### The value of the equation\n",
        "\n",
        "Q(s, a) is the total reward we can get by executing action a in state s, it can be obtained using V(s). This is the Q in Q-learning whose whole objective is to get values of Q for every pair of state and action.\n",
        "\n",
        "it can be defined as \n",
        "\n",
        "sum of for each state s' in S (probability of ending up in state s' by performing action a in state s * (reward for action a in state s + (gamma * value of state s')))\n",
        "\n",
        "![image.png](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_05_012.png)\n",
        "\n",
        "value of state s can also be expressed using Q function as \n",
        "\n",
        "![value using q function](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_05_013.png)\n",
        "\n",
        "and we can express Q function recursively which will be used extensively in Deep Q Learning as\n",
        "\n",
        "![q function recursive](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_05_014.png)\n",
        "\n",
        "Q values are much more convenient in practice than Value functions because it's simpler to make decisions about actions obtained from Q function than using value of state obtained from value function. at any given state, the agent just needs to calculate the q values of all available actions at that state and pick the one with the maximum q value. to do the same with value functions, it needs to know the transitional probabilities before hand which is very rare in practice, so we need to estimate the transitional probablities of every action and state pair. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhmc-yhkPjx_"
      },
      "source": [
        "### The Value Iteration method\n",
        "\n",
        "the above algorithm works well, but if there are any loops in the transitions, it leads to an infinite pool. \n",
        "\n",
        "in this case, we use value iteration algorithm. it works as follows:\n",
        "\n",
        "- initialize values of all states to some initial value, usually 0.\n",
        "- for every state s, perform the bellman update. \n",
        "![value iteration for value function](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_05_023.png)\n",
        "- repeat the above step for a large number of times or until the change becomes too small. \n",
        "\n",
        "the same algorithm can be used for q function as well. \n",
        "![value iteration for q function](https://static.packt-cdn.com/products/9781838826994/graphics/Images/B14854_05_024.png)\n",
        "\n",
        "the issue with this is that, the state space needs to be discrete and small enough to perform multiple iterations over all states. this is not an issue for small environments like frozenlake, but an issue with environments like cartpole, where the observation is four float values. we could split the space into bins as they dont extend to infinite in this case, but choosing the right bin size is another issue. this could be fixed by using neural networks in q learning. \n",
        "\n",
        "another issue with the value iteration approach is that we rarely know the transition probabilities of actions and rewards. the way gym works is, we observe the state, perform an action and only then we get the next observation and reward for the action performed. the only way to get the transition probabilties before hand is to cheat and look at the environment's code. \n",
        "\n",
        "so the only way to overcome this issue is to use the agent's history ie., keep track of what actions at what states gave how much reward. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6FFCb7ZsyR9"
      },
      "source": [
        "### Value Iteration in practice\n",
        "\n",
        "data stored:\n",
        "- reward table that stores state, action, target state\n",
        "- transitions table. if at state 0, we execute action 1 ten times where three times it takes us to state 4 and the rest of the times to state 5, in the table at the location (0,1), we store the dictionary {4:3, 5:7}. this denotes we go to state 4 after performing action 1 3 times and we go to state 5 after performing action 1 7 more times. \n",
        "- value table that stores the state and the calculated value of this state. \n",
        "\n",
        "we play 100 random steps from the environment updating the reward and transition table. then we perform value iteration loop over all states updating the value table. then we play several full episodes to check for improved values using value table. if average reward is above the 0.8 boundary for those test episodes, we stop training. we update reward and transition tables even during the testing phase. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ13pnCG6sVb",
        "outputId": "185727bd-2e7a-43cb-d745-9b2860073e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 122 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "p1pzxeNG6cs3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import collections\n",
        "from tensorboardX import SummaryWriter\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RSPvj0PH6l3d"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "  \"\"\"\n",
        "  This class keeps our tables and contains functions that will be used in the training loop\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Obtaining the initial observation using reset, creating the environment that will be \n",
        "    used for the data samples, define tables for rewards, transitions and value tables\n",
        "    \"\"\"\n",
        "    self.env = gym.make(ENV_NAME)\n",
        "    self.state = self.env.reset()\n",
        "    self.rewards = collections.defaultdict(float)\n",
        "    self.transits = collections.defaultdict(collections.Counter)\n",
        "    self.values = collections.defaultdict(float)\n",
        "\n",
        "  def play_n_random_steps(self, count):\n",
        "    \"\"\"\n",
        "    This function gathers random experience from the environment and update the reward and transition\n",
        "    tables. cross-entropy can only learn on full episodes, where as value iteration can learn on individual\n",
        "    steps/actions by remembering their outcomes. \n",
        "    \"\"\"\n",
        "    for _ in range(count):\n",
        "      action = self.env.action_space.sample()\n",
        "      new_state, reward, is_done, _ = self.env.step(action)\n",
        "      self.rewards[(self.state, action, new_state)] = reward\n",
        "      self.transits[(self.state, action)][new_state] += 1\n",
        "      if is_done:\n",
        "        self.state = self.env.reset()\n",
        "      else:\n",
        "        self.state = new_state\n",
        "\n",
        "  def calculate_action_value(self, state, action):\n",
        "    \"\"\"\n",
        "    calculates the value of action from state using the tables created in this class. this lets us select the\n",
        "    best action to perform from the state and calculate new value of state in value iteration\n",
        "    since the transition table is stored in the form of dict, we can use this to get the probabilities of \n",
        "    ending up in a state. \n",
        "    these probabilities are used in the bellman equation to get the q function which is the sum of rewards and \n",
        "    discounted value of target state multiplied with probablities. \n",
        "    \"\"\"\n",
        "    target_counts = self.transits[(state, action)]\n",
        "    total = sum(target_counts.values())\n",
        "    action_value = 0.0\n",
        "    for tgt_state, count in target_counts.items():\n",
        "      reward = self.rewards[(state, action, tgt_state)]\n",
        "      val = reward + GAMMA * self.values[tgt_state]\n",
        "      action_value += (count/total) * val\n",
        "    return action_value\n",
        "\n",
        "  def select_action(self, state):\n",
        "    \"\"\"\n",
        "    this uses the calculate_action_value() to make decision about the best action to take from a given state. \n",
        "    it iterates over all possible actions and picks the action with highest value    \n",
        "    \"\"\"\n",
        "    best_action, best_value = None, None\n",
        "    for action in range(self.env.action_space.n):\n",
        "      action_value = self.calculate_action_value(state, action)\n",
        "      if best_value is None or best_value < action_value:\n",
        "        best_value = action_value\n",
        "        best_action = action\n",
        "    return best_action\n",
        "  \n",
        "  def play_episode(self, env):\n",
        "    \"\"\"\n",
        "    this function uses select_action() to find the best action to take and plays one episode with the \n",
        "    environment. this is useful to play test episodes where we dont want to mess with current state of main\n",
        "    environment. \n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    state = env.reset()\n",
        "    while True:\n",
        "      action = self.select_action(state)\n",
        "      new_state, reward, is_done, _ = env.step(action)\n",
        "      self.rewards[(state, action, new_state)] = reward\n",
        "      self.transits[(state, action)][new_state] += 1\n",
        "      total_reward += reward\n",
        "      if is_done:\n",
        "        break\n",
        "      state = new_state\n",
        "    return total_reward \n",
        "  \n",
        "  def value_iteration(self):\n",
        "    \"\"\"\n",
        "    we loop over all states in the environment, for every state, calculate the values of states reachable \n",
        "    from it, then update value of current state with maximum value of the action available from the state\n",
        "    \"\"\"\n",
        "    for state in range(self.env.observation_space.n):\n",
        "      state_values = [\n",
        "                      self.calculate_action_value(state, action)\n",
        "                      for action in range(self.env.action_space.n)\n",
        "      ]\n",
        "      self.values[state] = max(state_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbG2KkRoJU28",
        "outputId": "9f282d11-40d1-476b-d94e-1145f58536ab"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  test_env = gym.make(ENV_NAME)\n",
        "  agent = Agent()\n",
        "  writer = SummaryWriter(comment=\"-v-iteration\")\n",
        "  iter_no = 0\n",
        "  best_reward = 0.0\n",
        "  while True:\n",
        "    iter_no += 1\n",
        "    # playing 100 random steps to fill transition and reward tables, then running value iteration\n",
        "    # over all states. \n",
        "    agent.play_n_random_steps(100)\n",
        "    agent.value_iteration()\n",
        "    # test episodes using the value table obtained so far. \n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "      reward += agent.play_episode(test_env)\n",
        "      reward /= TEST_EPISODES\n",
        "      writer.add_scalar(\"reward\", reward, iter_no)\n",
        "      if reward > best_reward:\n",
        "        print(f\"Best reward updated {best_reward:.3f} -> {reward:.3f}\")\n",
        "        best_reward = reward\n",
        "      if reward > 0.80:\n",
        "        print(f\"Solved in {iter_no} iteration\")\n",
        "        break\n",
        "  writer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYhHqxwFKUZM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM6YAlClKCo48pO6KgVTTDh",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Chapter 4 - Tabular Learning and the Bellman Equation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
