{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 3 - OpenAI Gym .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjcQ2yak4X/K/WpbIwBmsK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asrjy/ldrl/blob/main/Chapter%203%20-%20OpenAI%20Gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a basic Environment and Agent"
      ],
      "metadata": {
        "id": "YULtXuLOXt_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "vuR6pPtgXAUR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d89CRPt5UWvN"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "  \"\"\"\n",
        "  Defining an environment that will give agent random rewards for limited number of timesteps\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializing it's internal state. In this case, state is just a counter that limits the # of timesteps\n",
        "    that the agent is allowed to take to interact with the environment\n",
        "    \"\"\"\n",
        "    self.steps_left = 10\n",
        "  def get_observation(self):\n",
        "    \"\"\"\n",
        "    Returns the current environment's observation to the agent. This is usually implemented as some function\n",
        "    of the internal state of the environment\n",
        "    \"\"\"\n",
        "    return [0.0, 0.0, 0.0]\n",
        "  def get_actions(self):\n",
        "    \"\"\"\n",
        "    Allows the agent to query the set of actions it can execute. Normally, the set of actions the agent\n",
        "    can take do not change over time, but some actions are impossible to be executed in some states \n",
        "    example: tictactoe\n",
        "    \"\"\"\n",
        "    return [0, 1]\n",
        "  def is_done(self):\n",
        "    \"\"\"\n",
        "    Signals the end of the episode to the agent. \n",
        "    \"\"\"\n",
        "    return self.steps_left == 0\n",
        "  def action(self, action):\n",
        "    \"\"\"\n",
        "    Central piece of the environment's functionality. Does two things. Handles agent's action and returns\n",
        "    the reward for this action. In this example, reward is random and it's action is discarded. We update\n",
        "    the count of steps and refuse the continue the episodes if they are over. \n",
        "    \"\"\"\n",
        "    if self.is_done():\n",
        "      raise Exception(\"Game is Over\")\n",
        "    self.steps_left -= 1\n",
        "    return random.random()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Will keep the total reward accumulated by the agent during the episode\n",
        "    \"\"\"\n",
        "    self.total_reward = 0.0\n",
        "  def step(self, env):\n",
        "    \"\"\"\n",
        "    Accepts the environment's instance as argument and allows agent to \n",
        "     - observe the environment\n",
        "     - make decision about action to take based on observation made\n",
        "     - submit the action to environment\n",
        "     - get reward for the action taken\n",
        "    \"\"\"\n",
        "    current_obs = env.get_observation()\n",
        "    actions = env.get_actions()\n",
        "    reward = env.action(random.choice(actions))\n",
        "    self.total_reward += reward\n",
        "  "
      ],
      "metadata": {
        "id": "NixxVj46WTee"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  env = Environment()\n",
        "  agent = Agent()\n",
        "  while not env.is_done():\n",
        "    agent.step(env)\n",
        "  print(f\"Total reward: {agent.total_reward:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au8SKdA2XDjo",
        "outputId": "190129cc-fa40-4099-95e4-e484180e4e01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: 5.7481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Gym API\n",
        "\n",
        "Provides a rich collection of environments for RL Experiments. \n",
        "\n",
        "Central class in Library is called Env (environment). Instances of this class provide several methods and fields for various environments. \n",
        "\n",
        "But at a high level, every environment provides the following information. \n",
        "\n",
        "- Set of actions allowed to be executed in the environment. Gym supports both discrete and continuous actions. \n",
        "- Shape and boundaries of the observations, of the environment. \n",
        "- A method called \"step\" to execute an action which returns the current observation, reward and indication that the episode is over. \n",
        "- A method called \"reset\" which returns the environment to it's initial state and obtains the first observation. \n",
        "\n",
        "### Action space\n",
        "\n",
        "An environment is not limited to a single action. It can take multiple actions such as pushing multiple buttons simultaenously, modifying both heating and cooling setpoints at once etc., In such cases, Gym defines a special container class that allows the nesting of several action spaces into one unified action. \n",
        "\n",
        "### Observation space\n",
        "\n",
        "Observations are provided to the agent at every timestep besides the reward. They can be as simple as a bunch of numbers or as complex as several multidimensional tensors using images from multiple cameras. \n",
        "\n",
        "\n",
        "The basic abstract class Space includes two useful methods. \n",
        "\n",
        "- sample() -> returns random sample from the space\n",
        "- contains(x) -> checks whether the argument, 'x' belongs to the space's domain. \n",
        "\n",
        "Both these methods are abstract and reimplemented in each of the Space subclasses. \n",
        "\n",
        "- 'Discrete' subclass represents a mutually exclusive set of items numbered from 0 to n-1. It's only field is a number n, the count of items it describes\n",
        "- 'Box' subclass represents the n dimensional tensor of rational numbers with intervals [low, high]. For example, it could be used for an accelerator pedal with one value between low = 0.0 and high = 1.0\n",
        "\n",
        "    Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "    shape = (1,) denotes it's a scalar value. \n",
        "\n",
        "- 'Tuple' subclass allows us to combine several Space class instances together. This enables us to create action and observation spaces of any complexity. For example, a car could have lots of controls that can be changed at every timestamp. In this case, we can combine action spaces as \n",
        "\n",
        "    Tuple(spaces=(Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(n=3),Discrete(n=2)))\n",
        "    \n",
        "    This is rarely used. \n",
        "\n",
        "There are other subclasses but the preceeding three are the most important. All the subclasses implement the sample() and contains() methods. The sample() function performs a random sample corresponding to the Space class and parameters. The contains() method verifies that the given arguments comply with Space parameters and used to check an action's sanity. \n",
        "\n",
        "Every environment has two members of type Space: action_space and observation_space. This allows us to create generic code that could work with any environment. \n",
        "\n",
        "### The environment\n",
        "\n",
        "The environment is represented in Gym by the Env class. It has the following members. \n",
        "\n",
        "- action_space: This is the field of Space class and provides specification for allowed actions in the environment. \n",
        "- observation_space: This has the same Space class as action_space and specifies observations of the environment. \n",
        "- reset(): Resets environment to it's initial state returning the initial observation vector. \n",
        "- step(): Allows agent to take action and return information about the outcome of the action that are the next observation, reward and end of episode flag. "
      ],
      "metadata": {
        "id": "tS95FxGJXUE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an environment"
      ],
      "metadata": {
        "id": "mcw4SwyrbIeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym \n",
        "e = gym.make('CartPole-v0')"
      ],
      "metadata": {
        "id": "ZcbhFERQhlJD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THe observations in this env are 4 floating point numbers containing stick's center of mass, speed, angle to the platform and angular speed. \n",
        "\n",
        "The episode continues until stick falls, so we need to balance platform in a way to avoid the stick falling. "
      ],
      "metadata": {
        "id": "oQHPZB3gho_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs = e.reset()\n",
        "obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVCEAuBMh-Dj",
        "outputId": "670a6d0b-fd00-4629-f6e8-5ff9fbdfaec6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03792635, -0.04452565,  0.03030676, -0.02337004])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e.action_space, e.observation_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc4aSYzMiAC1",
        "outputId": "2f457021-9306-48b9-a3f3-2e36afee9898"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Discrete(2),\n",
              " Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e.step(0)\n",
        "# array(new_observation, reward, done_flag, additional_information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiMOXtpoiERS",
        "outputId": "0de14cbb-f95f-4de4-ec3d-11283ede2b40"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.03223446, -0.43560346,  0.03541374,  0.58066172]), 1.0, False, {})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e.action_space.sample()\n",
        "# This could be useful when we are not sure which action to take"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxRbzyWiiPKR",
        "outputId": "bd92c39b-03ca-4c6d-efba-4063e0e5a08d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e.observation_space.sample()\n",
        "# Not very useful"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "513Xm0uuigXl",
        "outputId": "3c838dbd-9b2b-4aec-edf1-943f0fe93887"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.5456860e+00,  1.8878092e+38, -3.9252144e-01, -2.3982668e+38],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  env = gym.make(\"CartPole-v0\")\n",
        "  total_reward = 0.0\n",
        "  total_steps = 1\n",
        "  obs = env.reset()\n",
        "  while True:\n",
        "    \"\"\"\n",
        "    Sampling random action, asking environment to execute it and return the next observation, reward and done flag. \n",
        "    If episode is over, we stop loop and show many steps we have taken and how much reward has been accumulated. \n",
        "    \"\"\"\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    total_steps += 1\n",
        "    if done: \n",
        "      break\n",
        "  print(f\"Episode done in {total_steps} steps. Total reward is {total_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgCXOh_rijTX",
        "outputId": "9d41d2d7-a4b9-4159-f1da-f39304dc66dc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode done in 17 steps. Total reward is 16.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g31kr9-flI3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}